{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SageMaker Pipelines from a Data Wrangler Flow\n",
    "\n",
    "You can use Amazon SageMaker Pipelines to create end-to-end workflows that manage and deploy SageMaker jobs. \n",
    "Pipelines come with SageMaker Python SDK integration, so you can build each step of your workflow using a \n",
    "Python-based interface.\n",
    "\n",
    "This notebook create a SageMaker pipeline that executes your Data Wrangler Flow `gsml-use-case-2-etl.flow` on the \n",
    "entire dataset as a data preparation step and optionally you can add additional steps to the pipeline.\n",
    "You will execute the pipeline and monitor its status using SageMaker Pipeline APIs.\n",
    "\n",
    "The pipeline will be processing data from the step `Join Tables` (**Step Output Name: default**). To save from a different step, go to Data Wrangler \n",
    "to select a new step to export. After your workflow is deployed, you can view the Directed Acyclic Graph\n",
    "(DAG) for your pipeline and manage your executions using Amazon SageMaker Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install SageMaker Autopilot Dependencies\n",
    "To use SageMaker Autopilot in your pipeline, you must install the newest versions of botocore, boto3, and sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U 'sagemaker>=2.118.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from packaging import version\n",
    "\n",
    "if version.parse(sagemaker.__version__) >= version.parse(\"2.118.0\"):\n",
    "    automl_enabled = True\n",
    "else:\n",
    "    automl_enabled = False\n",
    "    print(f\"The AutoML pipeline step requires sagemaker >= 2.118.0, but you have version {sagemaker.__version__}. The notebook uses the XGBoost step for training instead. To use the AutoML step, rerun the preceding cell and make sure the that you're using sagemaker 2.118.0 or later.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Outputs\n",
    "\n",
    "The below settings configure the inputs and outputs for the flow export.\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Configurable Settings </strong>\n",
    "\n",
    "In <b>Input - Source</b> you can configure the data sources that will be used as input by Data Wrangler\n",
    "\n",
    "1. For S3 sources, configure the source attribute that points to the input S3 prefixes\n",
    "2. For all other sources, configure attributes like query_string, database in the source's \n",
    "<b>DatasetDefinition</b> object.\n",
    "\n",
    "If you modify the inputs the provided data must have the same schema and format as the data used in the Flow. \n",
    "You should also re-execute the cells in this section if you have modified the settings in any data sources.\n",
    "\n",
    "Parametrized data sources will be ignored when creating ProcessingInputs, and will directly read from the source.\n",
    "Network isolation is not supported for parametrized data sources.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.dataset_definition.inputs import AthenaDatasetDefinition, DatasetDefinition, RedshiftDatasetDefinition\n",
    "\n",
    "data_sources = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input - S3 Source: part-00000-tid-3325758054719273621-f557a9df-d255-4241-bc37-5c43869c3044-879-8.c000.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources.append(ProcessingInput(\n",
    "    source=\"s3://dsoaws/nyc-taxi-orig-cleaned-split-parquet-per-year/ride-info/year=2019/part-00000-tid-3325758054719273621-f557a9df-d255-4241-bc37-5c43869c3044-879-8.c000.parquet\", # You can override this to point to other dataset on S3\n",
    "    destination=\"/opt/ml/processing/part-00000-tid-3325758054719273621-f557a9df-d255-4241-bc37-5c43869c3044-879-8.c000.parquet\",\n",
    "    input_name=\"part-00000-tid-3325758054719273621-f557a9df-d255-4241-bc37-5c43869c3044-879-8.c000.parquet\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input - S3 Source: part-00000-tid-4629508899230787795-13cdd6fc-f68a-4bbe-a8d9-b1f89f5baabd-1054-8.c000.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources.append(ProcessingInput(\n",
    "    source=\"s3://dsoaws/nyc-taxi-orig-cleaned-split-parquet-per-year/ride-fare/year=2019/part-00000-tid-4629508899230787795-13cdd6fc-f68a-4bbe-a8d9-b1f89f5baabd-1054-8.c000.parquet\", # You can override this to point to other dataset on S3\n",
    "    destination=\"/opt/ml/processing/part-00000-tid-4629508899230787795-13cdd6fc-f68a-4bbe-a8d9-b1f89f5baabd-1054-8.c000.parquet\",\n",
    "    input_name=\"part-00000-tid-4629508899230787795-13cdd6fc-f68a-4bbe-a8d9-b1f89f5baabd-1054-8.c000.parquet\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output: S3 settings\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Configurable Settings </strong>\n",
    "\n",
    "1. <b>bucket</b>: you can configure the S3 bucket where Data Wrangler will save the output. The default bucket from \n",
    "the SageMaker notebook session is used. \n",
    "2. <b>flow_export_id</b>: A randomly generated export id. The export id must be unique to ensure the results do not \n",
    "conflict with other flow exports \n",
    "3. <b>s3_ouput_prefix</b>:  you can configure the directory name in your bucket where your data will be saved.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Sagemaker session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# You can configure this with your own bucket name, e.g.\n",
    "# bucket = \"my-bucket\"\n",
    "bucket = sess.default_bucket()\n",
    "print(f\"Data Wrangler export storage bucket: {bucket}\")\n",
    "\n",
    "# unique flow export ID\n",
    "flow_export_id = f\"{time.strftime('%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\n",
    "flow_export_name = f\"flow-{flow_export_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the inputs required by the SageMaker Python SDK to launch a processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output name is auto-generated from the select node's ID + output name from the flow file.\n",
    "output_name = \"885d0017-0c4c-4151-8361-059fefc53fee.default\"\n",
    "\n",
    "s3_output_prefix = f\"export-{flow_export_name}/output\"\n",
    "s3_output_base_path = f\"s3://{bucket}/{s3_output_prefix}\"\n",
    "print(f\"Processing output base path: {s3_output_base_path}\\nThe final output location will contain additional subdirectories.\")\n",
    "\n",
    "processing_job_output = ProcessingOutput(\n",
    "    output_name=output_name,\n",
    "    source=\"/opt/ml/processing/output\",\n",
    "    destination=s3_output_base_path,\n",
    "    s3_upload_mode=\"EndOfJob\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Flow to S3\n",
    "\n",
    "To use the Data Wrangler as an input to the processing job,  first upload your flow file to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# name of the flow file which should exist in the current notebook working directory\n",
    "flow_file_name = \"gsml-use-case-2-etl.flow\"\n",
    "\n",
    "# Load .flow file from current notebook working directory \n",
    "!echo \"Loading flow file from current notebook working directory: $PWD\"\n",
    "\n",
    "with open(flow_file_name) as f:\n",
    "    flow = json.load(f)\n",
    "\n",
    "# Upload flow to S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(flow_file_name, bucket, f\"data_wrangler_flows/{flow_export_name}.flow\", ExtraArgs={\"ServerSideEncryption\": \"aws:kms\"})\n",
    "\n",
    "flow_s3_uri = f\"s3://{bucket}/data_wrangler_flows/{flow_export_name}.flow\"\n",
    "\n",
    "print(f\"Data Wrangler flow {flow_file_name} uploaded to {flow_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data Wrangler Flow is also provided to the Processing Job as an input source which we configure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input - Flow: gsml-use-case-2-etl.flow\n",
    "flow_input = ProcessingInput(\n",
    "    source=flow_s3_uri,\n",
    "    destination=\"/opt/ml/processing/flow\",\n",
    "    input_name=\"flow\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "\n",
    "# IAM role for executing the processing job.\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "# Unique processing job name. Give a unique name every time you re-execute processing jobs.\n",
    "processing_job_name = f\"data-wrangler-flow-processing-{flow_export_id}\"\n",
    "\n",
    "# Data Wrangler Container URL.\n",
    "container_uri = image_uris.retrieve(\"data-wrangler\", region, version=\"1.x\")\n",
    "# Pinned Data Wrangler Container URL.\n",
    "container_uri_pinned = \"663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.33.2\"\n",
    "\n",
    "# Processing Job Instance count and instance type.\n",
    "instance_count = 2\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "# Size in GB of the EBS volume to use for storing data during processing.\n",
    "volume_size_in_gb = 30\n",
    "\n",
    "# Content type for each output. Data Wrangler supports CSV as default and Parquet.\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "# Delimiter to use for the output if the output content type is CSV. Uncomment to set.\n",
    "# delimiter = \",\"\n",
    "\n",
    "# Compression to use for the output. Uncomment to set.\n",
    "# compression = \"gzip\"\n",
    "\n",
    "# Configuration for partitioning the output. Uncomment to set.\n",
    "# \"num_partition\" sets the number of partitions/files written in the output.\n",
    "# \"partition_by\" sets the column names to partition the output by.\n",
    "# partition_config = {\n",
    "#     \"num_partitions\": 1,\n",
    "#     \"partition_by\": [\"column_name_1\", \"column_name_2\"],\n",
    "# }\n",
    "\n",
    "# Network Isolation mode; default is off.\n",
    "enable_network_isolation = False\n",
    "\n",
    "# List of tags to be passed to the processing job.\n",
    "user_tags = []\n",
    "\n",
    "# Output configuration used as processing job container arguments. Only applies when writing to S3.\n",
    "# Uncomment to set additional configurations.\n",
    "output_config = {\n",
    "    output_name: {\n",
    "        \"content_type\": output_content_type,\n",
    "        # \"delimiter\": delimiter,\n",
    "        # \"compression\": compression,\n",
    "        # \"partition_config\": partition_config,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Refit configuration determines whether Data Wrangler refits the trainable parameters on the entire dataset. \n",
    "# When True, the processing job relearns the parameters and outputs a new flow file.\n",
    "# You can specify the name of the output flow file under 'output_flow'.\n",
    "# Note: There are length constraints on the container arguments (max 256 characters).\n",
    "refit_trained_params = {\n",
    "    \"refit\": False,\n",
    "    \"output_flow\": f\"data-wrangler-flow-processing-{flow_export_id}.flow\"\n",
    "}\n",
    "\n",
    "# KMS key for per object encryption; default is None.\n",
    "kms_key = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Configure Spark Cluster Driver Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Spark memory configuration. Change to specify the driver and executor memory in MB for the Spark cluster during processing.\n",
    "driver_memory_in_mb = 2048\n",
    "executor_memory_in_mb = 55742\n",
    "\n",
    "config = json.dumps({\n",
    "    \"Classification\": \"spark-defaults\",\n",
    "    \"Properties\": {\n",
    "        \"spark.driver.memory\": f\"{driver_memory_in_mb}m\",\n",
    "        \"spark.executor.memory\": f\"{executor_memory_in_mb}m\"\n",
    "    }\n",
    "})\n",
    "\n",
    "config_file = f\"config-{flow_export_id}.json\"\n",
    "with open(config_file, \"w\") as f:\n",
    "    f.write(config)\n",
    "\n",
    "config_s3_path = f\"spark_configuration/{processing_job_name}/configuration.json\"\n",
    "config_s3_uri = f\"s3://{bucket}/{config_s3_path}\"\n",
    "s3_client.upload_file(config_file, bucket, config_s3_path, ExtraArgs={\"ServerSideEncryption\": \"aws:kms\"})\n",
    "print(f\"Spark Config file uploaded to {config_s3_uri}\")\n",
    "os.remove(config_file)\n",
    "\n",
    "# Provides the spark config file to processing job and set the cluster driver memory. Uncomment to set.\n",
    "# data_sources.append(ProcessingInput(\n",
    "#     source=config_s3_uri,\n",
    "#     destination=\"/opt/ml/processing/input/conf\",\n",
    "#     input_name=\"spark-config\",\n",
    "#     s3_data_type=\"S3Prefix\",\n",
    "#     s3_input_mode=\"File\",\n",
    "#     s3_data_distribution_type=\"FullyReplicated\"\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Processer for the Pipeline\n",
    "\n",
    "Create a Processor object. The pipeline uses the processor to apply the transformations from your flow file to the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup processing job network configuration\n",
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "network_config = NetworkConfig(\n",
    "        enable_network_isolation=enable_network_isolation,\n",
    "        security_group_ids=None,\n",
    "        subnets=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    volume_size_in_gb=volume_size_in_gb,\n",
    "    network_config=network_config,\n",
    "    sagemaker_session=sess,\n",
    "    output_kms_key=kms_key,\n",
    "    tags=user_tags\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SageMaker Pipeline\n",
    "A SageMaker pipeline is composed of a series of steps. You begin by creating a processing step to \n",
    "transform your data. If youâ€™re using the notebook to train a model, you also define a training step. \n",
    "\n",
    "## Define Pipeline Steps\n",
    "To create a SageMaker pipeline, create a `ProcessingStep` using the Data Wrangler processor defined \n",
    "in the preceding section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "data_wrangler_step = ProcessingStep(\n",
    "    name=\"DataWranglerProcessingStep\",\n",
    "    processor=processor,\n",
    "    inputs=[flow_input] + data_sources, \n",
    "    outputs=[processing_job_output],\n",
    "    job_arguments=[f\"--output-config '{json.dumps(output_config)}'\"] \n",
    "        + [f\"--refit-trained-params '{json.dumps(refit_trained_params)}'\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add a `TrainingStep` to the pipeline. The step trains a model on the transformed dataset. By default, the notebook does not add a training step. To add a training step, set `add_training_step` to True.\n",
    "If you add a training step, you can choose between an AutoML training step and an XGBoost training step.\n",
    "You can also add more steps. For information about adding steps to a pipeline, see [Define a Pipeline](http://docs.aws.amazon.com/sagemaker/latest/dg/define-pipeline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_training_step = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the AutoML training step, run the following cells as-is. To use the XGBoost training step, set `use_automl_step` to False.\n",
    "\n",
    "Currently, the AutoML training step only supports the `ENSEMBLING` mode. For more information about the AutoML training step, \n",
    "see [Pipeline Steps](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-automl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_automl_step = automl_enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For AutoML, specifying the target column of your dataset is required. \n",
    "The target column is the column that the model is trained to predict. \n",
    "Provide the name of the target column in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_attribute_name = \"\"  # Provide the target column name here\n",
    "\n",
    "if use_automl_step and not target_attribute_name:\n",
    "    raise RuntimeError(\"You must specify the target column name.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_training_step and use_automl_step:\n",
    "    from sagemaker import AutoML, AutoMLInput\n",
    "    from sagemaker.workflow.automl_step import AutoMLStep\n",
    "    from sagemaker.workflow.functions import Join\n",
    "    from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "    pipeline_session = PipelineSession()\n",
    "\n",
    "    training_input_content_type = None\n",
    "\n",
    "    if output_content_type == \"CSV\":\n",
    "        training_input_content_type = 'text/csv;header=present'\n",
    "    elif output_content_type == \"Parquet\":\n",
    "        training_input_content_type = 'x-application/vnd.amazon+parquet'\n",
    "\n",
    "    auto_ml = AutoML(\n",
    "        role=iam_role,\n",
    "        target_attribute_name=target_attribute_name,\n",
    "        sagemaker_session=pipeline_session,\n",
    "        mode=\"ENSEMBLING\"\n",
    "    )\n",
    "\n",
    "    s3_input = Join(\n",
    "        on=\"/\",\n",
    "        values=[\n",
    "            data_wrangler_step.properties.ProcessingOutputConfig.Outputs[output_name].S3Output.S3Uri,\n",
    "            data_wrangler_step.properties.ProcessingJobName,\n",
    "            f'{output_name.replace(\".\", \"/\")}',\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_args = auto_ml.fit(\n",
    "        inputs=AutoMLInput(\n",
    "            inputs=s3_input,\n",
    "            content_type=training_input_content_type,\n",
    "            target_attribute_name=target_attribute_name\n",
    "        )\n",
    "    )\n",
    "\n",
    "    training_step = AutoMLStep(\n",
    "        name=\"DataWrangerAutoML\",\n",
    "        step_args=train_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost algorithm uses the first column as the target column. If this is not the case for \n",
    "the data that youâ€™ve processed, use the \"Move column\" transform to make the target column the \n",
    "first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_training_step and not use_automl_step:\n",
    "    from sagemaker.estimator import Estimator\n",
    "    from sagemaker.workflow.functions import Join\n",
    "\n",
    "    image_uri = sagemaker.image_uris.retrieve(\n",
    "        framework=\"xgboost\",\n",
    "        region=region,\n",
    "        version=\"1.5-1\",\n",
    "        py_version=\"py3\",\n",
    "        instance_type=instance_type,\n",
    "    )\n",
    "    xgb_train = Estimator(\n",
    "        image_uri=image_uri,\n",
    "        instance_type=instance_type,\n",
    "        instance_count=1,\n",
    "        role=iam_role,\n",
    "    )\n",
    "    xgb_train.set_hyperparameters(\n",
    "        objective=\"reg:squarederror\",\n",
    "        num_round=3,\n",
    "    )\n",
    "\n",
    "    from sagemaker.inputs import TrainingInput\n",
    "    from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "    xgb_input_content_type = None\n",
    "\n",
    "    if output_content_type == \"CSV\":\n",
    "        xgb_input_content_type = 'text/csv'\n",
    "    elif output_content_type == \"Parquet\":\n",
    "        xgb_input_content_type = 'application/x-parquet'\n",
    "\n",
    "    training_step = TrainingStep(\n",
    "        name=\"DataWrangerTrain\",\n",
    "        estimator=xgb_train,\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=Join(\n",
    "                    on=\"/\",\n",
    "                    values=[\n",
    "                        data_wrangler_step.properties.ProcessingOutputConfig.Outputs[output_name].S3Output.S3Uri,\n",
    "                        data_wrangler_step.properties.ProcessingJobName,\n",
    "                        f'{output_name.replace(\".\", \"/\")}',\n",
    "                    ]\n",
    "                ),\n",
    "                content_type=xgb_input_content_type\n",
    "            )\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Pipeline of Parameters, Steps\n",
    "Now you will create the SageMaker pipeline that combines the steps created above so it can be executed. \n",
    "\n",
    "Define Pipeline parameters that you can use to parametrize the pipeline. Parameters enable custom pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The parameters supported in this notebook includes:\n",
    "\n",
    "- `instance_type` - The ml.* instance type of the processing job.\n",
    "- `instance_count` - The instance count of the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "# Define Pipeline Parameters\n",
    "instance_type = ParameterString(name=\"InstanceType\", default_value=\"ml.m5.4xlarge\")\n",
    "instance_count = ParameterInteger(name=\"InstanceCount\", default_value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will create a pipeline with the steps and parameters defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# Create a unique pipeline name with flow export name\n",
    "pipeline_name = f\"pipeline-{flow_export_name}\"\n",
    "\n",
    "# Combine pipeline steps\n",
    "pipeline_steps = [data_wrangler_step]\n",
    "if add_training_step:\n",
    "    pipeline_steps.append(training_step)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[instance_type, instance_count],\n",
    "    steps=pipeline_steps,\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Examining the pipeline definition\n",
    "\n",
    "The JSON of the pipeline definition can be examined to confirm the pipeline is well-defined and \n",
    "the parameters and step properties resolve correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the pipeline to SageMaker and start execution\n",
    "\n",
    "Submit the pipeline definition to the SageMaker Pipeline service and start an execution. The role passed in \n",
    "will be used by the Pipeline service to create all the jobs defined in the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=iam_role)\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Operations: Examine and Wait for Pipeline Execution\n",
    "\n",
    "Describe the pipeline execution and wait for its completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Waiter will wait for up to 1 hour; increase the delay and max_attempts if necessary\n",
    "    execution.wait(delay=30, max_attempts=120)\n",
    "except Exception as e:\n",
    "    listed_steps = execution.list_steps()\n",
    "    errors = []\n",
    "    for step in listed_steps:\n",
    "        if \"FailureReason\" in step:\n",
    "            errors.append(step[\"FailureReason\"])\n",
    "    raise RuntimeError(str(errors)) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the steps in the execution. These are the steps in the pipeline that have been resolved by the step \n",
    "executor service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the pipeline execution status and details in Studio. For details please refer to \n",
    "[View, Track, and Execute SageMaker Pipelines in SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-studio.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangler Step S3 Output Location\n",
    "The output of your Data Wrangler processing step will be printed below. To prevent data of different processing jobs \n",
    "and different output nodes from being overwritten or combined, Data Wrangler uses the name of the processing job and \n",
    "the name of the output to write the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingJob\n",
    "\n",
    "data_wrangler_job_arn = execution.list_steps()[-1][\"Metadata\"][\"ProcessingJob\"][\"Arn\"]\n",
    "data_wrangler_job = ProcessingJob.from_processing_arn(sess, data_wrangler_job_arn)\n",
    "data_wrangler_job_name = data_wrangler_job.describe()[\"ProcessingJobName\"]\n",
    "\n",
    "s3_job_results_path = f\"{s3_output_base_path}/{data_wrangler_job_name}/{output_name.replace('.', '/')}\"\n",
    "print(f\"Job results are saved to S3 path: {s3_job_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Pipeline cleanup\n",
    "To delete the SageMaker resources created in this notebook, set `pipeline_deletion` to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_deletion = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipeline_deletion:\n",
    "    pipeline.delete()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
