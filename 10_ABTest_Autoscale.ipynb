{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B test, traffic shift, and autoscale across 2 model variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --disable-pip-version-check -q sagemaker==2.35.0\n",
    "!pip install --disable-pip-version-check -q jsonlines==3.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Ignore WARNING messages ^^ above ^^_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import botocore\n",
    "\n",
    "config = botocore.config.Config(user_agent_extra='dlai-pds/c3/w2')\n",
    "\n",
    "# Amazon SageMaker Python APIs\n",
    "sm = boto3.client(service_name='sagemaker', \n",
    "                  config=config)\n",
    "\n",
    "sm_runtime = boto3.client('sagemaker-runtime',\n",
    "                          config=config)\n",
    "\n",
    "# Amazon CloudWatch Python APIs\n",
    "cw = boto3.client(service_name='cloudwatch', \n",
    "                  config=config)\n",
    "\n",
    "# Amazon SageMaker Autoscaling Python APIs\n",
    "autoscale = boto3.client(service_name=\"application-autoscaling\", \n",
    "                         config=config)\n",
    "\n",
    "sess = sagemaker.Session(sagemaker_client=sm,\n",
    "                         sagemaker_runtime_client=sm_runtime)\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sess.boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create an endpoint with multiple variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model variant was trained to predict the sentiment of customer reviews into positive (1), neutral (0), or negative (-1).  \n",
    "\n",
    "The models are saved as .tar.gz files in the following S3 locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_s3_uri = 's3://dsoaws/workshop/models/ab/variant_a/model.tar.gz'\n",
    "model_b_s3_uri = 's3://dsoaws/workshop/models/ab/variant_b/model.tar.gz'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Specify the Docker image and instance type \n",
    "This example uses a Docker image which has PyTorch and TorchServe pre-installed. \n",
    "\n",
    "We use the `ml.m5.large` instance type which has 2 CPUs and 8 GB memory because it is a good balance of price and performance.  \n",
    "\n",
    "There are many instance types available [here](https://aws.amazon.com/sagemaker/pricing/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/endpoint-workflow-1-image.png\" width=\"60%\" align=\"center\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_instance_type = 'ml.m5.large'\n",
    "\n",
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework='pytorch', \n",
    "    version='1.6.0',\n",
    "    instance_type=inference_instance_type,\n",
    "    region=region,\n",
    "    py_version='py3',\n",
    "    image_scope='inference'\n",
    ")\n",
    "print('Docker image: {}'.format(inference_image_uri))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Register each model.tar.gz variant with SageMaker\n",
    "\n",
    "Create an Amazon SageMaker Model based on the `model_a_s3_uri` data.\n",
    "\n",
    "The `PrimaryContainer` includes the S3 location of each model.tar.gz (`ModelDataUrl` key), Docker image to use (`Image` key), number of instances (`initial_instance_count` key), and traffic-split percentage (`initial_weight` key) used to serve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/endpoint-workflow-2-models.png\" width=\"60%\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "model_name_a = '{}-{}'.format('a', timestamp)\n",
    "model_name_b = '{}-{}'.format('b', timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import production_variant\n",
    "\n",
    "model_a = sm.create_model(\n",
    "    ModelName=model_name_a,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        'ModelDataUrl': model_a_s3_uri, \n",
    "        'Image': inference_image_uri \n",
    "    }\n",
    ")\n",
    "pprint(model_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import production_variant\n",
    "\n",
    "model_b = sm.create_model(\n",
    "    ModelName=model_name_b, \n",
    "    ExecutionRoleArn=role, \n",
    "    PrimaryContainer={\n",
    "        'ModelDataUrl': model_b_s3_uri, \n",
    "        'Image': inference_image_uri\n",
    "    }\n",
    ")\n",
    "pprint(model_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Create a `production_variant` for each model\n",
    "\n",
    "The production variants will be compared directly in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/endpoint-workflow-3-variants.png\" width=\"60%\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variantA = production_variant(\n",
    "    model_name=model_name_a, \n",
    "    instance_type=inference_instance_type,\n",
    "    initial_weight=50,\n",
    "    initial_instance_count=1,\n",
    "    variant_name='VariantA',\n",
    ")\n",
    "print(variantA)\n",
    "\n",
    "variantB = production_variant(\n",
    "    model_name=model_name_b,\n",
    "    instance_type=inference_instance_type,\n",
    "    initial_weight=50, \n",
    "    initial_instance_count=1,\n",
    "    variant_name='VariantB'\n",
    ")\n",
    "print(variantB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Create the endpoint configuration\n",
    "\n",
    "Create the endpoint configuration by specifying the two production variants that you just configured.\n",
    "\n",
    "In this example, we will also enable [data capture](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-capture-endpoint.html) to log the prediction requests and responses from our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/endpoint-workflow-4-configuration.png\" width=\"60%\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = '{}-{}'.format('ab', timestamp)\n",
    "\n",
    "# Sampling percentage. Choose an integer value between 0 and 100\n",
    "initial_sampling_percentage = 20 \n",
    "\n",
    "# The S3 URI containing the captured data\n",
    "data_capture_s3_uri = 's3://{}/endpoint_data_capture/'.format(bucket)\n",
    "\n",
    "endpoint_config = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name, \n",
    "    ProductionVariants=[variantA, variantB],\n",
    "    DataCaptureConfig= {\n",
    "        'EnableCapture': True, \n",
    "        'InitialSamplingPercentage' : initial_sampling_percentage,\n",
    "        'DestinationS3Uri': data_capture_s3_uri,\n",
    "        'CaptureOptions': [\n",
    "            {'CaptureMode': 'Input'}, # capture requests\n",
    "            {'CaptureMode': 'Output'} # capture responses\n",
    "        ] \n",
    "    }\n",
    ")\n",
    "pprint(endpoint_config)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Create the SageMaker REST endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/endpoint-workflow-5-endpoint.png\" width=\"60%\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ab_endpoint_name = '{}-{}'.format('ab', timestamp)\n",
    "print('Endpoint name: {}'.format(model_ab_endpoint_name))\n",
    "\n",
    "endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=model_ab_endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print('Creating endpoint {}'.format(model_ab_endpoint_name))\n",
    "pprint(endpoint_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Review the created endpoint in the AWS console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">SageMaker REST endpoint</a></b>'.format(region, model_ab_endpoint_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Review `inference.py`\n",
    "\n",
    "Click here to review the code:  [inference.py](src/inference.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Wait for the endpoint to deploy (5-10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "waiter = sm.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=model_ab_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait until the ^^ endpoint ^^ is deployed_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Test the model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Test the model on a few sample product reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Amazon SageMaker `Predictor` class to access the deployed REST endpoint.\n",
    "\n",
    "The Predictor class uses using the `JSONLinesSerializer()` to converting json data to raw tensors for the request.  \n",
    "\n",
    "The Predictor class uses and `JSONLinesDeserializer()` to convert raw tensors back to json data for the response.  \n",
    "\n",
    "More information about the serializers can be found [here](https://sagemaker.readthedocs.io/en/stable/api/inference/serializers.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONLinesSerializer\n",
    "from sagemaker.deserializers import JSONLinesDeserializer\n",
    "\n",
    "inputs = [\n",
    "    {\"features\": [\"I love this product!\"]},\n",
    "    {\"features\": [\"OK, but not great.\"]},\n",
    "    {\"features\": [\"This is not the right product.\"]},\n",
    "]\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=model_ab_endpoint_name, \n",
    "    serializer=JSONLinesSerializer(), \n",
    "    deserializer=JSONLinesDeserializer(),\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "predicted_classes = predictor.predict(inputs)\n",
    "\n",
    "for idx, predicted_class in enumerate(predicted_classes):\n",
    "    print(\"Predicted sentiment [{}] for review {} with probability [{}]\".format(predicted_class['predicted_label'], inputs[idx]['features'], predicted_class['probability']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Prepare to review model system metrics from Amazon CloudWatch\n",
    "\n",
    "Let's analyze some of the model system metrics tracked by Amazon CloudWatch including the following:\n",
    "* CPU Utilization\n",
    "* Latency\n",
    "* Number of Invocations. \n",
    "\n",
    "The full list of metrics can be found [here](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html).\n",
    "\n",
    "But before that, let's create a function that will help to extract the results from CloudWatch and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_endpoint_metrics_for_variants(endpoint_name, \n",
    "                                       namespace_name, \n",
    "                                       metric_name, \n",
    "                                       variant_names, \n",
    "                                       start_time, \n",
    "                                       end_time):\n",
    "    \n",
    "    try:\n",
    "        joint_variant_metrics = None\n",
    "\n",
    "        for variant_name in variant_names:\n",
    "            metrics = cw.get_metric_statistics( # extracts the results in a dictionary format\n",
    "                Namespace=namespace_name, # the namespace of the metric, e.g. \"AWS/SageMaker\"\n",
    "                MetricName=metric_name, # the name of the metric, e.g. \"CPUUtilization\"\n",
    "                StartTime=start_time, # the time stamp that determines the first data point to return\n",
    "                EndTime=end_time, # the time stamp that determines the last data point to return\n",
    "                Period=60, # the granularity, in seconds, of the returned data points\n",
    "                Statistics=[\"Sum\"], # the metric statistics\n",
    "                Dimensions=[ # dimensions, as CloudWatch treats each unique combination of dimensions as a separate metric\n",
    "                    {\"Name\": \"EndpointName\", \"Value\": endpoint_name}, \n",
    "                    {\"Name\": \"VariantName\", \"Value\": variant_name}\n",
    "                ],\n",
    "            )\n",
    "            \n",
    "            if metrics[\"Datapoints\"]: # access the results from the distionary using the key \"Datapoints\"\n",
    "                df_metrics = pd.DataFrame(metrics[\"Datapoints\"]) \\\n",
    "                    .sort_values(\"Timestamp\") \\\n",
    "                    .set_index(\"Timestamp\") \\\n",
    "                    .drop(\"Unit\", axis=1) \\\n",
    "                    .rename(columns={\"Sum\": variant_name}) # rename the column with the metric results as a variant_name\n",
    "                \n",
    "                if joint_variant_metrics is None:\n",
    "                    joint_variant_metrics = df_metrics\n",
    "                else:\n",
    "                    joint_variant_metrics = joint_variant_metrics.join(df_metrics, how=\"outer\")\n",
    "        \n",
    "        joint_variant_metrics.plot(title=metric_name)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "# Establish wide enough time bounds to show all the charts using the same timeframe:\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_time = datetime.now() - timedelta(minutes=30)\n",
    "end_time = datetime.now() + timedelta(minutes=30)\n",
    "\n",
    "print('Start Time: {}'.format(start_time))\n",
    "print('End Time: {}'.format(end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Run some predictions and view the metrics for each variant (1-2 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(0, 100):\n",
    "    predicted_classes = predictor.predict(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Μake sure the predictions ^^ above ^^ ran successfully (1-2 minutes, please be patient.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compare the system metrics between model variants. \n",
    "\n",
    "If you see `Metrics not yet available`, please be patient as metrics may take a bit mins to appear in CloudWatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(30) # Sleep to accomodate a slight delay in metrics gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the list of the the variant names to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_names = [variantA[\"VariantName\"], variantB[\"VariantName\"]]\n",
    "\n",
    "print(variant_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPUUtilization\n",
    "# The sum of each individual CPU core's utilization. \n",
    "# The CPU utilization of each core can range between 0 and 100. For example, if there are four CPUs, CPUUtilization can range from 0% to 400%.\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, \n",
    "    namespace_name=\"/aws/sagemaker/Endpoints\", \n",
    "    metric_name=\"CPUUtilization\",\n",
    "    variant_names=variant_names,\n",
    "    start_time=start_time,\n",
    "    end_time=end_time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelLatency\n",
    "# The interval of time taken by a model to respond as viewed from SageMaker (in microseconds).\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, \n",
    "    namespace_name=\"AWS/SageMaker\", \n",
    "    metric_name=\"ModelLatency\",\n",
    "    variant_names=variant_names,\n",
    "    start_time=start_time,\n",
    "    end_time=end_time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invocations\n",
    "# The number of requests sent to a model endpoint.\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, \n",
    "    namespace_name=\"AWS/SageMaker\", \n",
    "    metric_name=\"Invocations\",\n",
    "    variant_names=variant_names,\n",
    "    start_time=start_time,\n",
    "    end_time=end_time    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Review model requests/responses from data capture S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "    \n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}?region={}&prefix=endpoint_data_capture/{}/\">S3 Data Capture</a></b>'.format(bucket, region, model_ab_endpoint_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Copy the requests/responses locally to inspect\n",
    "\n",
    "We will use a super-useful library called `jsonlines` to handle full json documents separated by newlines.  \n",
    "\n",
    "For more information on `jsonlines`, click [here](https://jsonlines.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $data_capture_s3_uri ./endpoint_data_capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jsonlines\n",
    "from glob import glob\n",
    "import base64\n",
    "\n",
    "pd.set_option('max_colwidth', 500)\n",
    "\n",
    "glob_pattern = 'endpoint_data_capture/*/*/*/*/*/*/*.jsonl'\n",
    "for json_filename in glob(glob_pattern):\n",
    "    json_data = jsonlines.open(json_filename) \n",
    "\n",
    "    df = pd.json_normalize(json_data)[['captureData.endpointInput.data', 'captureData.endpointOutput.data']]\n",
    "    df = df.applymap(lambda encoded_data: base64.b64decode(encoded_data).decode())\n",
    "    df = df.append(df, ignore_index=True)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare to shift all traffic to variant B\n",
    "\n",
    "Let's assume that model variant B is the better variant based on some business metrics like more conversions, less churn, etc.  \n",
    "\n",
    "In this case, we want to shift all traffic to variant B **without downtime**, so we set the `DesiredWeight` for variant A to 0% and variant B to 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_endpoint_config = [\n",
    "    {\n",
    "        \"VariantName\": variantA[\"VariantName\"],\n",
    "        \"DesiredWeight\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"VariantName\": variantB[\"VariantName\"],\n",
    "        \"DesiredWeight\": 100,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Update the endpoint with the new variant weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=model_ab_endpoint_name, \n",
    "    DesiredWeightsAndCapacities=updated_endpoint_config \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Wait for the ^^ endpoint update ^^ to complete above (few minutes, please be patient.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Review the endpoint status in the AWS console.\n",
    "\n",
    "1. open the link below\n",
    "2. notice that you are in the section Amazon SageMaker -> Endpoints\n",
    "3. check the name of the endpoint, its ARN and status (`Updating` or `InService`)\n",
    "4. below you can see the endpoint runtime settings with the updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">SageMaker REST endpoint</a></b>'.format(region, model_ab_endpoint_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = sm.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(EndpointName=model_ab_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Run some more predictions and view the metrics for each variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(0, 100):\n",
    "    predicted_classes = predictor.predict(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Μake sure the predictions ^^ above ^^ ran successfully (1-2 minutes, please be patient.)_\n",
    "\n",
    "If you see `Metrics not yet available`, please be patient as metrics may take a few minutes to appear in CloudWatch. Compare the results with the plots above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPUUtilization\n",
    "\n",
    "# The sum of each individual CPU core's utilization. \n",
    "# The CPU utilization of each core can range between 0 and 100. For example, if there are four CPUs, CPUUtilization can range from 0% to 400%.\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, \n",
    "    namespace_name=\"/aws/sagemaker/Endpoints\",\n",
    "    metric_name=\"CPUUtilization\",\n",
    "    variant_names=variant_names,\n",
    "    start_time=start_time,\n",
    "    end_time=end_time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelLatency\n",
    "\n",
    "# The interval of time taken by a model to respond as viewed from SageMaker (in microseconds).\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, \n",
    "    namespace_name=\"AWS/SageMaker\", \n",
    "    metric_name=\"ModelLatency\",\n",
    "    variant_names=variant_names,\n",
    "    start_time=start_time,\n",
    "    end_time=end_time    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invocations\n",
    "\n",
    "# The number of requests sent to a model endpoint.\n",
    "plot_endpoint_metrics_for_variants(\n",
    "    endpoint_name=model_ab_endpoint_name, \n",
    "    namespace_name=\"AWS/SageMaker\", \n",
    "    metric_name=\"Invocations\",\n",
    "    variant_names=variant_names,\n",
    "    start_time=start_time,\n",
    "    end_time=end_time    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Autoscale to handle more model-prediction traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Configure autoscaling for variant B\n",
    "For variant B, we specify the `MinCapacity` indicates the minimum number of instances and  `MaxCapacity` is the maximum number of instances for your endpoint. \n",
    "\n",
    "In this case, we always have at least 1 instance running and a maximum of 2 during peak periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoscale.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=\"endpoint/\" + model_ab_endpoint_name + \"/variant/VariantB\",\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=2,\n",
    "    RoleARN=role,\n",
    "    SuspendedState={\n",
    "        \"DynamicScalingInSuspended\": False,\n",
    "        \"DynamicScalingOutSuspended\": False,\n",
    "        \"ScheduledScalingSuspended\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = sm.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(EndpointName=model_ab_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Define and apply a scaling policy\n",
    "\n",
    "`TargetTrackingScaling` is the scaling policy uses a scaling metric and a target value as the indicator to scale. \n",
    "\n",
    "`PredefinedMetricSpecification` is the number of invocations on your instance.\n",
    "\n",
    "`TargetValue` is the number of invocations per ML instance you want to allow before triggering your scaling policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoscale.put_scaling_policy(\n",
    "    PolicyName=\"bert-reviews-autoscale-policy\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=\"endpoint/\" + model_ab_endpoint_name + \"/variant/VariantB\",\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"TargetValue\": 2.0, # the number of invocations per ML instance you want to allow before triggering your scaling policy\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerVariantInvocationsPerInstance\", # scaling metric\n",
    "        },\n",
    "        \"ScaleOutCooldown\": 60, # wait time, in seconds, before beginning another scale out activity after last one completes\n",
    "        \"ScaleInCooldown\": 300, # wait time, in seconds, before beginning another scale in activity after last one completes\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = sm.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(EndpointName=model_ab_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Generate more traffic (1-2 minutes, please be patient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(0, 100):\n",
    "    predicted_classes = predictor.predict(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Review the autoscaling in action\n",
    "\n",
    "1. open the link\n",
    "2. notice that you are in the section Amazon SageMaker -> Endpoints\n",
    "3. below you can see the endpoint runtime settings with the instance counts.\n",
    "\n",
    "You can run the predictions multiple times to observe the increase of the instance count to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/endpoints/{}\">SageMaker REST endpoint</a></b>'.format(region, model_ab_endpoint_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
